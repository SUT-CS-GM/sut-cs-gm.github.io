{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Course Overview","text":"<p>Welcome to the Fall 2025 offering of the Deep Generative Models course at SUT! We are excited to have you join us on this journey into one of the most dynamic and foundational fields in modern artificial intelligence.</p> <ul> <li>University: Sharif University of Technology (SUT)</li> <li>Department: Department of Mathematical Sciences</li> <li>Group: Computer Science (CS)</li> </ul> Course Description (Click to Expand) <p>This course provides a comprehensive, in-depth introduction to the principles, algorithms, and applications of deep generative modeling. We will explore a wide array of foundational and state-of-the-art architectures, beginning with core probabilistic concepts.</p> <p>Key topics are structured around three main pillars:</p> <ul> <li>Foundational Generative Models: Including Autoregressive Models, Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Normalizing Flows, and Energy-Based Models (EBMs).</li> <li>Modern Diffusion &amp; Score-Based Models: A deep dive into Score-Based Models (SDEs), Flow Matching, and Diffusion Models, including their ODE/Flow formulations and applications like text-to-image generation.</li> <li>Large Language Models (LLMs): A significant module dedicated to the principles of LLMs and LMMs, their emergent abilities, and their capacity for reasoning.</li> </ul> Learning Objectives (Click to Expand) <p>Upon successful completion of this course, students will be able to:</p> <ul> <li>Understand the fundamental principles and theoretical underpinnings of major generative model families.</li> <li>Implement and train deep generative models for practical applications and problem domains.</li> <li>Master the core concepts and mathematical foundations required to analyze and compare different model architectures.</li> <li>Develop the theoretical knowledge necessary to read, understand, and critically assess current research papers in the field.</li> </ul>"},{"location":"#instructor","title":"Instructor","text":"<ul> <li> <p>Dr. Fatemeh Seyyedsalehi</p> <p>Instructor</p> <p>seyyedsalehi@sharif.edu</p> <p> </p> </li> </ul>"},{"location":"#tentative-schedule","title":"Tentative Schedule","text":""},{"location":"#lectures","title":"Lectures","text":"# Topic of Session Material Date 1 Introduction Topic 1 \u06f2\u06f0 \u0645\u0647\u0631 2 Intro. to Probabilistic Graphical Models Topic 2 \u06f2\u06f2 \u0645\u0647\u0631 3 Intro. to Probabilistic Graphical Models Topic 3 \u06f2\u06f7 \u0645\u0647\u0631 4 Intro. to Probabilistic Graphical Models Topic 4 \u06f2\u06f9 \u0645\u0647\u0631 5 Autoregressive Models Topic 5 \u06f4 \u0622\u0628\u0627\u0646 6 Autoregressive Models Topic 5 \u06f6 \u0622\u0628\u0627\u0646 7 VAEs Topic 6 \u06f1\u06f1 \u0622\u0628\u0627\u0646 8 VAEs Topic 6 \u06f1\u06f3 \u0622\u0628\u0627\u0646 9 GANs Topic 7 \u06f1\u06f8 \u0622\u0628\u0627\u0646 10 GANs Topic 7 \u06f2\u06f0 \u0622\u0628\u0627\u0646 11 Normalizing Flows - Invertible Models \u06f2\u06f5 \u0622\u0628\u0627\u0646 12 Energy Based Models \u06f2\u06f7 \u0622\u0628\u0627\u0646 13 Score Based Models and SDEs \u06f2 \u0622\u0630\u0631 14 Score Based Models and SDEs \u06f4 \u0622\u0630\u0631 15  Midterm Exam \u06f9 \u0622\u0630\u0631 16 Flow Matching \u06f1\u06f1 \u0622\u0630\u0631 17 Intro. to Diffusion Models \u06f1\u06f6 \u0622\u0630\u0631 18 Diffusion Models and ODE/Flows \u06f1\u06f8 \u0622\u0630\u0631 19 Diffusion Models and ODE/Flows \u06f2\u06f3 \u0622\u0630\u0631 20 Text-to-Image Generation with Diffusion Models \u06f2\u06f5 \u0622\u0630\u0631 21 Diffusion for Discrete Data \u06f3\u06f0 \u0622\u0630\u0631 22 Advanced Topics in Generative Models \u06f2 \u062f\u06cc 23 Intro. to LLMs and LMMs \u06f7 \u062f\u06cc 24 LLM Emergent Abilities \u06f9 \u062f\u06cc 25 LLM Emergent Abilities \u06f1\u06f4 \u062f\u06cc 26 Reasoning in LLMs \u06f1\u06f6 \u062f\u06cc 27 Reasoning in LLMs \u06f2\u06f1 \u062f\u06cc 28 Reasoning in LLMs \u06f2\u06f3 \u062f\u06cc"},{"location":"#homeworks","title":"Homeworks","text":"Homework Release Deadline Homework 1: PGMs, ARs \u06f6 \u0622\u0628\u0627\u0646 \u06f2\u06f3 \u0622\u0628\u0627\u0646 Homework 2: VAEs, GANs, Flows \u06f2\u06f5 \u0622\u0628\u0627\u0646 \u06f1\u06f0 \u0622\u0630\u0631 Homework 3: EBMs, SDEs, Flow Matching \u06f1\u06f1 \u0622\u0630\u0631 \u06f2\u06f9 \u0622\u0630\u0631 Homework 4: Diffusion, ODEs \u06f3\u06f0 \u0622\u0630\u0631 \u06f1\u06f5 \u062f\u06cc Homework 5: LLMs, LMMs \u06f1\u06f6 \u062f\u06cc \u06f3\u06f0 \u062f\u06cc"},{"location":"#logistics-policies","title":"Logistics &amp; Policies","text":"<ul> <li> <p>Lectures: Held on Sunday and Tuesday from 10:30 to 12:30 in the Department of Mathematical Sciences, classroom 202.</p> </li> <li> <p>Late Policy: You have a total budget of 15 slack days for the semester, which can be used for any homework (practical or theoretical) without penalty.</p> <ul> <li>There is a maximum limit of 5 slack days per assignment. Submissions after 5 days will not be accepted, as solutions may be released.</li> <li>Once your 15-day total budget is exhausted, any further late submission (within the 5-day window) will be penalized 2% of the assignment's grade for every hour of delay.</li> </ul> </li> <li> <p>Collaboration &amp; Resources: Collaboration on assignments is permitted. However, your final submission must be written entirely by you. You must clearly cite the names of any collaborators and list all external resources (outside of course materials) that you used.</p> </li> <li> <p>Practical Assignments: Practical (coding) assignments will include an oral defense. You will not receive a grade for the practical portion if you cannot demonstrate sufficient mastery of your code during the defense.</p> </li> <li> <p>Support: You can ask questions on Telegram Group or email the course instructor or head TA for office hours.</p> </li> </ul>"},{"location":"#grading","title":"Grading","text":"<p>The grading for the Deep Generative Models course is structured as follows:</p> Assessment Component Points Homeworks (5 practical &amp; conceptual HWs) 10 + [1 Extra Point] Midterm 4 Final 6 Total 20 + [1 Extra Point]"},{"location":"#head-assistant","title":"Head Assistant","text":"<ul> <li> <p>Maryam Rezaee</p> <p>Head TA</p> <p>ms.maryamrezaee@gmail.com</p> <p> </p> </li> </ul>"},{"location":"#teaching-assistants","title":"Teaching Assistants","text":"<ul> <li> <p> <p>Firoozeh Abrishami</p> <p>Teaching Assistant</p> <p>f.abrishami110@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Shaygan Adim</p> <p>Teaching Assistant</p> <p>sh83adim@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Mohammad Ali Banayeeanzade</p> <p>Teaching Assistant</p> <p>a.banayeaab@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Mahshid Dehghani</p> <p>Teaching Assistant</p> <p>da.mahshid@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Ali Salimi</p> <p>Teaching Assistant</p> <p>iamlalisalimil@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Mohammad Shirkhani</p> <p>Teaching Assistant</p> <p>muhammad.shirkhani@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Pooriya Safaei</p> <p>Teaching Assistant</p> <p>pooriya.safaei.80@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Amir Mohammad Fakhimi</p> <p>Teaching Assistant</p> <p>fakhimi.amirmohamad@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Sara Karimi</p> <p>Teaching Assistant</p> <p>sarraah.karimi@gmail.com</p> <p> </p> </p> </li> <li> <p> <p>Ramtin Moslemi</p> <p>Teaching Assistant</p> <p>ramtin.moslemi@yahoo.com</p> <p> </p> </p> </li> <li> <p> <p>Mohamad Hosein Mehdikhani</p> <p>Teaching Assistant</p> <p>mhms2003bzm@gmail.com</p> <p> </p> </p> </li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"exams/final/","title":"Fall 2025 Final","text":""},{"location":"exams/midterm/","title":"Fall 2025 Midterm","text":""},{"location":"homeworks/","title":"Homeworks","text":""},{"location":"homeworks/#general-policies-submission","title":"General Policies &amp; Submission","text":"<p>Please read the following policies carefully before submitting your work. These rules apply to all assignments.</p> <ul> <li>Platform: All homeworks must be submitted via the official class Quera Page.</li> <li>Deadline: Submissions are due by 23:59 on the specified deadline date.</li> <li>Format: All files (theoretical PDFs, practical notebooks, etc.) must be compressed and submitted as a single <code>.zip</code> file. Please follow the naming conventions specified in the assignment files (e.g., <code>HW_T_[STD_ID].pdf</code>).</li> <li>Late Policy: You have a total budget of 15 slack days for the semester.<ul> <li>A maximum of 5 slack days can be used per assignment.</li> <li>Submissions after 5 days will not be accepted (as solutions may be released).</li> <li>Once your 15-day budget is used, late submissions (within the 5-day window) will be penalized 2% of the grade per hour.</li> </ul> </li> <li>Collaboration: Collaboration is permitted, but your final submission must be written entirely by you. You must cite the names of collaborators and any external resources used.</li> <li>Practical Defense: Practical (coding) assignments include a mandatory oral defense. You must be able to demonstrate a clear understanding of your code to receive a grade.</li> <li>Support: Ask all questions on the Quera assignment page or in the class Telegram Group.</li> </ul>"},{"location":"homeworks/#homework-1-pgms-ars","title":"Homework 1: PGMs, ARs","text":"<ul> <li>Deadline: 23 Aban 1404 (23:59)</li> <li>TAs Responsible:<ul> <li>Theoretical: Firoozeh Abrishami</li> <li>Practical: Shaygan Adim</li> </ul> </li> </ul> <p>Download Assignment</p>"},{"location":"homeworks/#homework-2-vaes-gans-flows","title":"Homework 2: VAEs, GANs, Flows","text":"<ul> <li>Deadline: 10 Azar 1404 (23:59)</li> <li>TAs Responsible:<ul> <li>VAEs: Amir Mohammad Fakhimi</li> <li>GANs, Flows: Mahshid Dehghani</li> </ul> </li> </ul> <p>Download Assignment</p>"},{"location":"material/","title":"Course Material","text":"<p>This page hosts all lecture slides and supplementary reading materials for each topic covered in the course.</p>"},{"location":"material/#topic-1-introduction","title":"Topic 1: Introduction","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material","title":"Supplementary Material","text":"<ul> <li>A Tutorial on Deep Generative Models</li> </ul>"},{"location":"material/#topic-2-pgm-representation","title":"Topic 2: PGM Representation","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_1","title":"Supplementary Material","text":"<ul> <li>Probabilistic Graphical Models (Textbook [4]), Chapters 3 &amp; 4</li> <li>CS228 (Probabilistic Graphical Models) Notes</li> </ul>"},{"location":"material/#topic-3-pgm-inference","title":"Topic 3: PGM Inference","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_2","title":"Supplementary Material","text":"<ul> <li>Probabilistic Machine Learning (Textbook [2]), Chapter 7.4</li> <li>Probabilistic Graphical Models (Textbook [4]), Chapter 9</li> <li>CS228 (Probabilistic Graphical Models) Notes</li> </ul>"},{"location":"material/#topic-4-pgm-learning","title":"Topic 4: PGM Learning","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_3","title":"Supplementary Material","text":"<ul> <li>Probabilistic Graphical Models (Textbook [4]), Chapters 16 &amp; 17</li> <li>A Note on the EM Algorithm</li> <li>CS228 (Probabilistic Graphical Models) Notes</li> </ul>"},{"location":"material/#topic-5-autoregressive-models","title":"Topic 5: Autoregressive Models","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_4","title":"Supplementary Material","text":"<ul> <li>Probabilistic Machine Learning (Textbook [2]), Chapter 22</li> <li>Harvard NLP Tutorial: The Annotated Transformer</li> <li>CSE599i: Neural Autoregressive Density Estimation (NADE)</li> <li>CSE599i: Transformers</li> <li>Self-Attention from Scratch (Sebastian Raschka)</li> <li>Introduction to Positional Encoding (Machine Learning Mastery)</li> </ul>"},{"location":"material/#topic-6-variational-autoencoder","title":"Topic 6: Variational Autoencoder","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_5","title":"Supplementary Material","text":"<ul> <li>Probabilistic Machine Learning (Textbook [2]), Chapter 21</li> <li>CSE599i: VAE Notes</li> <li>CS236 (Deep Generative Models): VAE Notes</li> </ul>"},{"location":"material/#topic-7-gan","title":"Topic 7: GAN","text":"<p>Download Slides</p> Click to view slides in browser"},{"location":"material/#supplementary-material_6","title":"Supplementary Material","text":"<ul> <li>Probabilistic Machine Learning (Textbook [2]), Chapter 26</li> <li>CSE599i: GAN Notes</li> <li>Original GAN Paper (Goodfellow et al.)</li> <li>WGAN (Wasserstein GAN) Paper</li> <li>WGAN-GP (Improved WGAN) Paper</li> </ul>"},{"location":"resources/","title":"Resources","text":"<p>This page contains a list of recommended textbooks and similar courses from other institutions for further study.</p>"},{"location":"resources/#recommended-books","title":"Recommended Books","text":"<ol> <li> <p>Deep Learning: Foundations and Concepts</p> <ul> <li>Bishop, Christopher M. and Hugh Bishop</li> <li>Springer</li> </ul> </li> <li> <p>Probabilistic Machine Learning: Advanced Topics</p> <ul> <li>Murphy, Kevin P.</li> <li>The MIT Press</li> </ul> </li> <li> <p>Deep Generative Modeling</p> <ul> <li>Tomczak, Jakub M.</li> <li>Springer</li> </ul> </li> <li> <p>Probabilistic Graphical Models: Principles and Techniques</p> <ul> <li>Koller, Daphne and Nir Friedman</li> <li>The MIT Press</li> </ul> </li> </ol>"},{"location":"resources/#related-university-courses","title":"Related University Courses","text":"<ul> <li>Stanford CS-236: Deep Generative Models</li> <li>CMU 18-789: Deep Generative Modeling</li> <li>Washington CSE-599: Generative Models</li> <li>Berkeley CS 294-158: Deep Unsupervised Learning</li> </ul>"}]}